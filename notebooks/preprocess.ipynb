{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset(\"openai/gsm8k\", \"main\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, a = dataset[\"train\"][0][\"question\"], dataset[\"train\"][0][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_tokens = tokenizer(q, return_tensors=\"pt\")\n",
    "qna_tokens = tokenizer(q + \" Answer : \" + a, return_tensors=\"pt\", padding=\"max_length\", max_length=128, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000,     45,   4306,    689,   6216,  27203,    311,    220,   2166,\n",
       "            315,   1077,   4885,    304,   5936,     11,    323,   1243,   1364,\n",
       "           6216,   4376,    439,   1690,  27203,    304,   3297,     13,   2650,\n",
       "           1690,  27203,   1550,  42701,    689,   4662,  31155,    304,   5936,\n",
       "            323,   3297,     30,  22559,    551,  42701,    689,   6216,    220,\n",
       "           2166,     14,     17,    284,   1134,   2166,     14,     17,     28,\n",
       "           1187,   2511,   1187,  27203,    304,   3297,    627,     45,   4306,\n",
       "            689,   6216,    220,   2166,     10,   1187,    284,   1134,   2166,\n",
       "             10,   1187,     28,   5332,   2511,   5332,  27203,  31155,    304,\n",
       "           5936,    323,   3297,    627,    827,    220,   5332, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001, 128001,\n",
       "         128001, 128001]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qna_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf841c106bc4ded923afc3a7b1b50e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95c7de09c2e245ffb8c3c0d9314501dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_func(exp):\n",
    "    # Concatenate Question and Answer\n",
    "    text = exp[\"question\"] + \" \" + exp[\"answer\"]\n",
    "    # Tokenize the text\n",
    "    tokenized = tokenizer(text, return_tensors=\"pt\")\n",
    "    # Get the attention mask\n",
    "    attn_mask = tokenized[\"attention_mask\"]\n",
    "    # Get the length of the input\n",
    "    inp_len = attn_mask.sum()\n",
    "    \n",
    "    \n",
    "    # Attn mask for only the future token of the answer\n",
    "res = dataset.map(test_func, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Natalia sold 48/2 = <<48/2=24>>24 clips in May.\\nNatalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n#### 72'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['train']['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['train']['question'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['train']['x'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for i in range(len(dataset[\"train\"])):\n",
    "    q = dataset[\"train\"][i]['question']\n",
    "    a = dataset[\"train\"][i]['answer']\n",
    "    res.append(q + \" \" + a)\n",
    "    if i > 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_res = tokenizer(res, padding=\"max_length\", max_length=512, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token_res.input_ids), len(token_res.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e976cc255048b1a3db18751c0fc310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "590b81bfa0404796844dbe1b7176972e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecc5d853f2a4c3692392e2ccf5b0649",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/10 shards):   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fbeaf6964764ae3a86fa77a617dbed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Function to concatenate and tokenize input\n",
    "def preprocess_function(examples):\n",
    "    # Concatenate question and answer\n",
    "    inputs = [q + \" \" + a for q, a in zip(examples['question'], examples['answer'])]\n",
    "    \n",
    "    # Tokenize the input sequences\n",
    "    tokenized_inputs = tokenizer(inputs, padding='max_length', truncation=True, return_tensors=\"pt\")\n",
    "    \n",
    "    # Create attention masks: the answer portion should have causal masking\n",
    "    attention_mask = tokenized_inputs.attention_mask.clone()  # Base attention mask\n",
    "    \n",
    "    for i, input_ids in enumerate(tokenized_inputs.input_ids):\n",
    "        # Identify where the answer begins (we assume a simple space separation)\n",
    "        q_len = len(tokenizer.tokenize(examples['question'][i]))\n",
    "        a_len = len(tokenizer.tokenize(examples['answer'][i]))\n",
    "        \n",
    "        # Set attention masking so that tokens in the answer part only attend to previous answer tokens\n",
    "        # Tokens before the answer can attend freely\n",
    "        for j in range(q_len, q_len + a_len):\n",
    "            attention_mask[i, j+1:] = 0  # Mask future answer tokens\n",
    "\n",
    "    # Return tokenized inputs with attention masks\n",
    "    return {\n",
    "        \"input_ids\": tokenized_inputs.input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Save the tokenized dataset\n",
    "tokenized_dataset.save_to_disk(\"gsm8k_tokenized\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenized dataset\n",
    "tokenized_dataset = datasets.load_from_disk(\"gsm8k_tokenized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset['train']['input_ids'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
