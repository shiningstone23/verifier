{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifier의 성능 확인\n",
    "\n",
    "Verifier가 선택한 답이 맞을 확률이 높은 것이 맞는지?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir(\"../\")\n",
    "import sys; sys.path.append('scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "import torch\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    \n",
    ")\n",
    "from tqdm import tqdm\n",
    "from utils import HF_NAME_MAP\n",
    "from utils import set_seed, init_tokenizer, validate_args, _extract_answer\n",
    "\n",
    "config_path = \"configs/basic.yml\"\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "\n",
    "def get_likelihood(model, prompt_tokens, label_tokens):\n",
    "    \"\"\"\n",
    "    Compute the likelihood for multiple label tokens given a shared prompt.\n",
    "\n",
    "    Args:\n",
    "        model: The causal language model.\n",
    "        prompt_tokens (torch.Tensor): The prompt tokens of shape (1, q_tokens).\n",
    "        label_tokens (torch.Tensor): The label tokens of shape (n_samples, a_tokens).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (n_samples,) containing the log-likelihood for each sample.\n",
    "    \"\"\"\n",
    "    n_samples = label_tokens.size(0)\n",
    "    q_tokens = prompt_tokens.size(1)\n",
    "\n",
    "    # Repeat the prompt tokens for each label\n",
    "    repeated_prompt_tokens = prompt_tokens.repeat(n_samples, 1).to(label_tokens.device)  # Shape: (n_samples\n",
    "\n",
    "    # Concatenate prompt and label tokens\n",
    "    input_tokens = torch.cat([repeated_prompt_tokens, label_tokens], dim=1)  # Shape: (n_samples, q_tokens + a_tokens)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tokens)\n",
    "        logits = outputs.logits.detach().cpu()  # Shape: (n_samples, seq_length, vocab_size)\n",
    "\n",
    "    # Extract logits corresponding to label tokens\n",
    "    label_start_idx = q_tokens  # Labels start after the prompt\n",
    "    label_logits = logits[:, label_start_idx - 1:-1, :]  # Shape: (n_samples, a_tokens, vocab_size)\n",
    "\n",
    "    # Compute log-probabilities for the label tokens\n",
    "    log_probs = torch.log_softmax(label_logits, dim=-1)  # Shape: (n_samples, a_tokens, vocab_size)\n",
    "    label_log_probs = log_probs.gather(2, label_tokens.unsqueeze(-1)).squeeze(-1)  # Shape: (n_samples, a_tokens)\n",
    "\n",
    "    # Sum log-probabilities over all label tokens for each sample\n",
    "    # total_log_likelihood = label_log_probs.sum(dim=1)  # Shape: (n_samples,)\n",
    "    total_log_likelihood = label_log_probs.mean(dim=1)  # Shape: (n_samples,)\n",
    "\n",
    "    return total_log_likelihood.detach().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sft_llama-1b\"\n",
    "task_name = \"gsm8k\"\n",
    "model_type, pt_name = model_name.split(\"_\")\n",
    "hf_name = HF_NAME_MAP[pt_name]\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_name)\n",
    "init_tokenizer(tokenizer)\n",
    "\n",
    "dset = load_from_disk(\"data/ver_sft_llama-1b_gsm8k/test\")\n",
    "\n",
    "# verifier_path = f\"models/veri_{model_name}_{task_name}\"\n",
    "# verifier_path = f\"models/{model_name}_{task_name}\"\n",
    "paths = [\n",
    "    \"/home/chanwoo/chanwoo/repo/verifier/models/verifier/checkpoints/veri_sft_llama-1b_gsm8k/checkpoint-13149/target\",\n",
    "    f\"models/{model_name}_{task_name}\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b69f7ae7174072a752e98b21004ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# verifier_path = \"/home/chanwoo/chanwoo/repo/verifier/models/verifier/checkpoints/veri_sft_llama-8b_gsm8k/checkpoint-3600/target\"\n",
    "# verifier_path = \"models/verifier/checkpoints/veri_sft_llama-1b_gsm8k/checkpoint-13149/target\"\n",
    "# verifier_path = \"/home/chanwoo/chanwoo/repo/verifier/models/verifier/checkpoints/veri_sft_llama-1b_gsm8k/checkpoint-13149/target\"\n",
    "verifier_path = \"/home/chanwoo/chanwoo/repo/verifier/models/veri_sft_llama-8b_gsm8k/target\"\n",
    "verifier = AutoModelForCausalLM.from_pretrained(\n",
    "    verifier_path,\n",
    "    quantization_config=BitsAndBytesConfig(**config['qt']),\n",
    "    **config['model'][pt_name]\n",
    ")\n",
    "\n",
    "\n",
    "# gen_model = AutoModelForCausalLM.from_pretrained(\n",
    "#     f\"models/{model_name}_{task_name}\",\n",
    "#     quantization_config=BitsAndBytesConfig(**config['qt']),\n",
    "#     **config['model'][pt_name]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing started\n"
     ]
    }
   ],
   "source": [
    "def parse_log(file_path, start_time_str):\n",
    "    start_time = datetime.strptime(start_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "    results = []\n",
    "\n",
    "    with open(file_path, \"r\") as log_file:\n",
    "        lines = log_file.readlines()\n",
    "\n",
    "    parsing = False\n",
    "    current_entry = {}\n",
    "\n",
    "    for line in lines:\n",
    "        # Check if we should start parsing after the specific time\n",
    "        match_time = re.match(r\"\\[(.*?)\\]\", line)\n",
    "        if match_time:\n",
    "            log_time = datetime.strptime(match_time.group(1), \"%Y-%m-%d %H:%M:%S,%f\")\n",
    "            if log_time >= start_time and \"Starting Evaluate script\" in line:\n",
    "                parsing = True\n",
    "                print(\"Parsing started\")\n",
    "\n",
    "\n",
    "        if not parsing:\n",
    "            continue\n",
    "\n",
    "        # Parse Question\n",
    "        elif \"[INFO] - Question:\" in line:\n",
    "            question_match = re.search(r\"Question: (.*)\", line)\n",
    "            if question_match:\n",
    "                current_entry[\"Question\"] = eval(question_match.group(1))  # Safely parse list\n",
    "\n",
    "        # Parse Prediction\n",
    "        elif \"[INFO] - Prediction:\" in line and \"INFO\" in line:\n",
    "            prediction_match = re.search(r\"Prediction: (.*)\", line)\n",
    "            if prediction_match:\n",
    "                current_entry[\"Prediction\"] = eval(prediction_match.group(1))  # Safely parse list\n",
    "\n",
    "        # Parse Answer\n",
    "        elif \"[INFO] - Answer:\" in line:\n",
    "            answer_match = re.search(r\"Answer: (.*)\", line)\n",
    "            if answer_match:\n",
    "                current_entry[\"Answer\"] = eval(answer_match.group(1))  # Safely parse list\n",
    "\n",
    "        # If all fields are collected, save the entry and reset\n",
    "        if all(key in current_entry for key in [\"Question\", \"Prediction\", \"Answer\"]):\n",
    "            results.append(current_entry)\n",
    "            current_entry = {}\n",
    "\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "log_path = \"logs/test_verifier-8b.log\"\n",
    "start_time = \"2024-12-11 14:25:34\"\n",
    "parsed_data = parse_log(log_path, start_time)\n",
    "\n",
    "res = []\n",
    "for entry in parsed_data:\n",
    "    # print(\"Best Answer Index:\", entry[\"Best answer index\"])\n",
    "    # print(\"Question:\", entry[\"Question\"])\n",
    "    # print(\"Prediction:\", entry[\"Prediction\"])\n",
    "    # print(\"Answer:\", entry[\"Answer\"])\n",
    "    # print(\"-\" * 80)\n",
    "    # break\n",
    "    try:\n",
    "        answer_pat = r'####\\s*\\d+'\n",
    "        res.append({\n",
    "            'question': entry[\"Question\"],\n",
    "            'prediction': entry[\"Prediction\"],\n",
    "            'answer': entry[\"Answer\"],\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'Ken created a care package to send to his brother, who was away at boarding school.  Ken placed a box on a scale, and then he poured into the box enough jelly beans to bring the weight to 2 pounds.  Then, he added enough brownies to cause the weight to triple.  Next, he added another 2 pounds of jelly beans.  And finally, he added enough gummy worms to double the weight once again.  What was the final weight of the box of goodies, in pounds?',\n",
       " 'chosen': 'In pounds, the box started at <<0=0>>0.\\nKen added enough jelly beans to cause the weight to rise to 2 pounds, so now the weight was 2 pounds.\\nA tripled weight, which is equal to 2 pounds, is 2*3=<<2*3=6>>6 pounds.\\nKen then added another 2 pounds so now the weight was 6+2=<<2+6=8>>8 pounds.\\nThen, he added enough gummy worms to double the weight again, 8 pounds, so the final weight becomes 8*2=<<8*2=16>>16 pounds.\\n#### 16 pounds',\n",
       " 'rejected': 'All told, 2 pounds of jelly beans were added to the care package, and the brownies and gummy worms each added 6 pounds, for a total of 12 pounds.  Thus the care package contained 14 pounds, which is the answer.\\n####14'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = load_from_disk(f\"data/ver_sft_llama-8b_gsm8k/train\")\n",
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = train_dataset['prompt'][100]\n",
    "chosen = train_dataset['chosen'][100]\n",
    "rejected = train_dataset['rejected'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-127.2425)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_answer = tokenizer(prompt + chosen, return_tensors=\"pt\").input_ids\n",
    "chosen_token = tokenizer(chosen, return_tensors=\"pt\").input_ids\n",
    "logits = verifier(prompt_answer).logits.detach().cpu()\n",
    "label_len = chosen_token.shape[1]\n",
    "torch.gather(logits[:,-label_len:-1,:].log_softmax(-1), dim=2, index=chosen_token[:,1:].unsqueeze(2)).squeeze(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-187.8689)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_answer = tokenizer(prompt + rejected, return_tensors=\"pt\").input_ids\n",
    "rejected_token = tokenizer(rejected, return_tensors=\"pt\").input_ids\n",
    "logits = verifier(prompt_answer).logits.detach().cpu()\n",
    "label_len = rejected_token.shape[1]\n",
    "torch.gather(logits[:,-label_len:-1,:].log_softmax(-1), dim=2, index=rejected_token[:,1:].unsqueeze(2)).squeeze(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 24\n",
    "instruction = \"Judge whether the reasoning from given question is reasonable?\\nQuestion: \"\n",
    "\n",
    "def is_answer(answer, pred):\n",
    "    pn, gn = _extract_answer(pred, answer)\n",
    "    return pn == gn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peter has 4 boxes with the same number of chocolate bars in each, while Martha has 7 boxes with the same number of chocolate bars in each. If Peter and Martha have totals of 64 and 56 chocolate bars respectively, how many more chocolate bars does Peter have in each box than Martha?\n",
      "\n",
      "Peter has 64 chocolate bars in 4 equal boxes so there are 64/4 = <<64/4=16>>16 bars in each box\n",
      "Martha has 56 chocolate bars in 7 equal boxes so there are 56/7 = <<56/7=8>>8 bars in each box\n",
      "Peter has 16-8 = <<16-8=8>>8 bars more than Martha in each box\n",
      "#### 8\n"
     ]
    }
   ],
   "source": [
    "print(entry[\"Question\"][0])\n",
    "print()\n",
    "print(entry[\"Answer\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Answer: Since for each box Martha has 56/7=<<56/7=8>>8 chocolate bars\n",
      "That means in each box Martha has 8-4=<<8-4=4>>4 fewer chocolate bars than in each box does Peter\n",
      "#### 4 fewer\n"
     ]
    }
   ],
   "source": [
    "print(entry[\"Prediction\"][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [41:59, 25.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "exp_res = []\n",
    "for idx, entry in tqdm(enumerate(parsed_data[:100])):\n",
    "    is_answers = [is_answer(entry[\"Answer\"][0], pred) for pred in entry[\"Prediction\"]]\n",
    "    total_num_answer = sum(is_answers)\n",
    "    questions = [instruction + entry[\"Question\"][0] + pred for pred in entry[\"Prediction\"]]\n",
    "\n",
    "    # Step 2: Tokenize 입력 배치\n",
    "    batch_inputs = tokenizer(questions, return_tensors='pt', padding=True, padding_side='right')\n",
    "    attention_mask = batch_inputs[\"attention_mask\"]\n",
    "\n",
    "\n",
    "    # Step 3: Tokenize 정답 레이블 (패딩 포함)\n",
    "    label_tokens = tokenizer(entry[\"Prediction\"], return_tensors=\"pt\", padding=True, padding_side='right')\n",
    "\n",
    "    # Step 4: Verifier 모델 호출 (배치 처리)\n",
    "    logits = verifier(batch_inputs.input_ids, attention_mask=attention_mask).logits.detach().cpu()\n",
    "\n",
    "    # Step 5: Mask 생성 및 로그 확률 계산\n",
    "    label_mask = (label_tokens.input_ids != tokenizer.pad_token_id)  # 패딩이 아닌 부분은 True\n",
    "    shifted_labels = label_tokens.input_ids[:, 1:]  # 첫 번째 토큰 제외 (Decoder 방식)\n",
    "\n",
    "    log_probs = torch.gather(\n",
    "        logits[:, -shifted_labels.shape[1]-1:, :].log_softmax(-1),  # Label 길이에 맞춘 logits\n",
    "        dim=2,\n",
    "        index=shifted_labels.unsqueeze(2)  # 레이블 차원 확장\n",
    "    ).squeeze(2)  # [Batch, Sequence]\n",
    "\n",
    "    # Mask 적용 후, 각 문장의 총 로그 확률 계산\n",
    "    masked_log_probs = log_probs * label_mask[:, 1:].float()  # 첫 번째 패딩 제외\n",
    "    total_log_probs = masked_log_probs.sum(dim=1)  # 배치별 로그 확률 합계\n",
    "\n",
    "    exp_res.append({\n",
    "        \"is_answer\": is_answers,\n",
    "        \"total_num_answer\": total_num_answer,\n",
    "        \"total_log_probs\": total_log_probs,\n",
    "        \"questions\": questions,\n",
    "        \"label_tokens\": label_tokens,\n",
    "        \"logits\": logits,\n",
    "        \"log_probs\": log_probs,\n",
    "        \"masked_log_probs\": masked_log_probs,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexp_res.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexp_res\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/storage.py:1006\u001b[0m, in \u001b[0;36mTypedStorage.__reduce__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__reduce__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1005\u001b[0m     b \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO()\n\u001b[0;32m-> 1006\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_use_new_zipfile_serialization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1007\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (_load_from_bytes, (b\u001b[38;5;241m.\u001b[39mgetvalue(),))\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/serialization.py:656\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[0;32m--> 656\u001b[0m         \u001b[43m_legacy_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/torch/serialization.py:800\u001b[0m, in \u001b[0;36m_legacy_save\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    798\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m serialized_storage_keys:\n\u001b[1;32m    799\u001b[0m     storage, dtype \u001b[38;5;241m=\u001b[39m serialized_storages[key]\n\u001b[0;32m--> 800\u001b[0m     \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_write_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_should_read_directly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_element_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"exp_res.pkl\", \"wb\") as f:\n",
    "    pickle.dump(exp_res, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = 0\n",
    "for i in range(100):\n",
    "    ans_ind = exp_res[i]['total_log_probs'].argmax() \n",
    "    acc += exp_res[i]['is_answer'][ans_ind]\n",
    "acc / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_res[0]['total_log_probs'].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gen_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m label_tokens \u001b[38;5;241m=\u001b[39m tokenizer(entry[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Step 4: Verifier 모델 호출 (배치 처리)\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mgen_model\u001b[49m(batch_inputs\u001b[38;5;241m.\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\u001b[38;5;241m.\u001b[39mlogits\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Step 5: Mask 생성 및 로그 확률 계산\u001b[39;00m\n\u001b[1;32m     17\u001b[0m label_mask \u001b[38;5;241m=\u001b[39m (label_tokens\u001b[38;5;241m.\u001b[39minput_ids \u001b[38;5;241m!=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)  \u001b[38;5;66;03m# 패딩이 아닌 부분은 True\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gen_model' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Instruction 준비\n",
    "instruction = \"\"\n",
    "questions = [instruction + entry[\"Question\"][0] + pred for pred in entry[\"Prediction\"]]\n",
    "\n",
    "# Step 2: Tokenize 입력 배치\n",
    "batch_inputs = tokenizer(questions, return_tensors='pt', padding=True, padding_side='right')\n",
    "attention_mask = batch_inputs[\"attention_mask\"]\n",
    "\n",
    "\n",
    "# Step 3: Tokenize 정답 레이블 (패딩 포함)\n",
    "label_tokens = tokenizer(entry[\"Prediction\"], return_tensors=\"pt\", padding=True, padding_side='right')\n",
    "\n",
    "# Step 4: Verifier 모델 호출 (배치 처리)\n",
    "logits = gen_model(batch_inputs.input_ids, attention_mask=attention_mask).logits.detach().cpu()\n",
    "\n",
    "# Step 5: Mask 생성 및 로그 확률 계산\n",
    "label_mask = (label_tokens.input_ids != tokenizer.pad_token_id)  # 패딩이 아닌 부분은 True\n",
    "shifted_labels = label_tokens.input_ids[:, 1:]  # 첫 번째 토큰 제외 (Decoder 방식)\n",
    "\n",
    "log_probs = torch.gather(\n",
    "    logits[:, -shifted_labels.shape[1]-1:, :].log_softmax(-1),  # Label 길이에 맞춘 logits\n",
    "    dim=2,\n",
    "    index=shifted_labels.unsqueeze(2)  # 레이블 차원 확장\n",
    ").squeeze(2)  # [Batch, Sequence]\n",
    "\n",
    "# Mask 적용 후, 각 문장의 총 로그 확률 계산\n",
    "masked_log_probs = log_probs * label_mask[:, 1:].float()  # 첫 번째 패딩 제외\n",
    "total_log_probs = masked_log_probs.sum(dim=1)  # 배치별 로그 확률 합계\n",
    "print(total_log_probs)\n",
    "print(total_log_probs.argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instruction = \"\"\n",
    "# # instruction = \"Judge whether the reasoning from given question is reasonable?\\nQuestion: \"\n",
    "# for idx in range(len(entry[\"Prediction\"])):\n",
    "#     prompt_answer = tokenizer(instruction + entry[\"Question\"][0] + entry[\"Prediction\"][idx], return_tensors='pt').input_ids\n",
    "#     label_token = tokenizer(entry[\"Prediction\"][idx], return_tensors=\"pt\").input_ids\n",
    "#     logits = verifier(prompt_answer).logits.detach().cpu()\n",
    "#     label_len = label_token.shape[1]\n",
    "#     logp = torch.gather(logits[:,-label_len:-1,:].log_softmax(-1), dim=2, index=label_token[:,1:].unsqueeze(2)).squeeze(2).sum()\n",
    "    \n",
    "#     # logp = 0\n",
    "#     # for i in range(1, label_token.shape[1]):\n",
    "#     #     logp += logits[:, -label_len-1+i, :].log_softmax(-1)[:,label_token[0,i]]\n",
    "#     # logp\n",
    "#     print(idx, logp)\n",
    "#     # break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
