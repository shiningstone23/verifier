{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os; os.chdir(\"../\")\n",
    "import sys; sys.path.append(\"scripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import yaml\n",
    "from utils import set_seed, init_tokenizer\n",
    "from utils import HF_NAME_MAP\n",
    "from utils import RegexStopAndExtractCriteria\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoModelForCausalLM, AutoTokenizer,\n",
    "    GenerationConfig, \n",
    "    StoppingCriteriaList, LogitsProcessorList,\n",
    "    MaxTimeCriteria, LogitsProcessor, StoppingCriteria\n",
    ")\n",
    "\n",
    "config_path = 'configs/basic.yml'\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "set_seed(config['seed'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets, load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dset = load_from_disk(\"/home/chanwoo/chanwoo/repo/verifier/data/sft_llama-8b_gsm8k_correct_100_0_\")\n",
    "# new_q = tokenizer.batch_decode(dset['question'], skip_special_tokens=True)\n",
    "# new_q2 = [q.split(\"Question: \")[1] for q in new_q]\n",
    "# new_dset = {\n",
    "#     \"question\": new_q2,\n",
    "#     \"answer\": dset['answer']\n",
    "# }\n",
    "# new_dset_obj = Dataset.from_dict(new_dset)\n",
    "# new_dset_obj.save_to_disk(\"data/sft_llama-8b_gsm8k_correct_100_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Answer the question below. The final answer should be number and write the final answer after the \\n####. For example, \\n#### 000\n",
    "Question : Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Validate the solution for the question below.\n",
    "Question : Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers' market?\n",
    "Answer: She eats 3 x 16 = <<3*16=48>>48 fresh duck eggs per day.\\nShe sells 48 x 2 = <<48*2=96>>96 fresh duck eggs at the farmers' market.\\nShe makes 96 x $2 = $<<96*2=192>>192 every day at the farmers' market.\\n#### 192\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import subprocess\n",
    "# from transformers import BitsAndBytesConfig, QuantoConfig\n",
    "# test_qt_configs = [\n",
    "#     BitsAndBytesConfig(load_in_4bit=True),\n",
    "#     BitsAndBytesConfig(load_in_8bit=True),\n",
    "#     QuantoConfig(weights=\"int8\"),\n",
    "# ]\n",
    "\n",
    "# def get_gpu_memory_by_pid(pid):\n",
    "#     try:\n",
    "#         # nvidia-smi로 프로세스 정보 조회\n",
    "#         result = subprocess.run(\n",
    "#             [\"nvidia-smi\", \"--query-compute-apps=pid,used_memory\", \"--format=csv,noheader,nounits\"],\n",
    "#             stdout=subprocess.PIPE, text=True\n",
    "#         )\n",
    "\n",
    "#         # 결과 파싱\n",
    "#         processes = result.stdout.strip().split(\"\\n\")\n",
    "#         for process in processes:\n",
    "#             process_info = process.split(\", \")\n",
    "#             process_pid = int(process_info[0])\n",
    "#             used_memory = int(process_info[1])  # GPU 메모리 사용량 (MB 단위)\n",
    "\n",
    "#             if process_pid == pid:\n",
    "#                 return used_memory\n",
    "#         return None  # 해당 PID가 GPU에서 실행 중이지 않음\n",
    "\n",
    "#     except FileNotFoundError:\n",
    "#         print(\"nvidia-smi 명령어를 찾을 수 없습니다. NVIDIA 드라이버가 설치되어 있는지 확인하세요.\")\n",
    "#         return None\n",
    "#     except Exception as e:\n",
    "#         print(f\"에러 발생: {e}\")\n",
    "#         return None\n",
    "\n",
    "# # 예시: 특정 PID의 GPU 메모리 사용량 확인\n",
    "# pid = 5765  # 확인하려는 PID\n",
    "# gpu_memory = get_gpu_memory_by_pid(pid)\n",
    "# if gpu_memory is not None:\n",
    "#     print(f\"PID {pid}의 GPU 메모리 사용량: {gpu_memory} MB\")\n",
    "# else:\n",
    "#     print(f\"PID {pid}는 GPU에서 실행 중이지 않습니다.\")\n",
    "\n",
    "# model_name = \"sft_llama-1b\"\n",
    "# task_name = \"gsm8k\"\n",
    "# model_path = f\"models/{model_name}_{task_name}\"\n",
    "# model_type, pt_name = model_name.split(\"_\")\n",
    "# hf_name = HF_NAME_MAP[pt_name]\n",
    "\n",
    "# pid = os.getpid()\n",
    "\n",
    "# for qt in test_qt_configs:\n",
    "#     model = AutoModelForCausalLM.from_pretrained(\n",
    "#         model_path,\n",
    "#         quantization_config=qt,\n",
    "#         **config['model'][pt_name]\n",
    "#     )\n",
    "#     gpu_memory = get_gpu_memory_by_pid(pid)\n",
    "#     print(f\"QT: {qt}, GPU Memory: {gpu_memory} MB\")\n",
    "#     model.cpu()\n",
    "#     del model\n",
    "\n",
    "#     torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"sft_llama-1b\"\n",
    "task_name = \"gsm8k\"\n",
    "model_path = f\"models/{model_name}_{task_name}\"\n",
    "\n",
    "model_type, pt_name = model_name.split(\"_\")\n",
    "hf_name = HF_NAME_MAP[pt_name]\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_name)\n",
    "init_tokenizer(tokenizer)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    # model_path,\n",
    "    \"models/sft_llama-1b_gsm8k_star_0\",\n",
    "    # quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n",
    "    **config['model'][pt_name]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Please calculate the solution step-by-step and conclude the answer with \\n#### followed by the result.\\n\"\n",
    "query = instruction + \"'A fruit vendor bought 50 watermelons for $80. He sold all of them at a profit of 25%. How much was each watermelon sold? Answer: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token = tokenizer(query, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, pattern, tokenizer):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pattern (str): 탐지할 정규식 패턴.\n",
    "            tokenizer (Tokenizer): 토크나이저 객체.\n",
    "        \"\"\"\n",
    "        self.pattern = pattern\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_ids (torch.Tensor): (n_samples, seq_len) 형태의 입력 토큰 ID.\n",
    "            scores (torch.Tensor): (n_samples, n_vocab) 형태의 로그 확률 점수.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: 수정된 로그 확률 점수.\n",
    "        \"\"\"\n",
    "        pad_token = self.tokenizer.pad_token_id\n",
    "\n",
    "        for i in range(input_ids.size(0)):\n",
    "            # 전체 시퀀스를 디코딩\n",
    "            decoded = self.tokenizer.decode(input_ids[i])\n",
    "            # 패턴이 디코딩된 텍스트에서 발견되면 처리\n",
    "            if re.search(self.pattern, decoded):\n",
    "                scores[i, :] = -1e9  # 모든 토큰 비활성화\n",
    "                scores[i, pad_token] = 0  # pad_token만 활성화\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "res = model.generate(\n",
    "    inputs=input_token.input_ids.to(model.device),\n",
    "    generation_config=GenerationConfig(\n",
    "        max_length=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        num_beams=1,\n",
    "        num_return_sequences=16\n",
    "    ),\n",
    "    logits_processor=LogitsProcessorList([\n",
    "        CustomLogitsProcessor(pattern=r\"####\\s*-?\\d+(\\D)\", tokenizer=tokenizer)\n",
    "    ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 205])\n",
      "0: 80 x 25/100 = <<80*25/100=20>>20% of the watermelons were sold.\n",
      "So, 80 - 20 = <<80-20=60>>60% of the watermelons were not sold.\n",
      "Therefore, 60 / 100 x 50 = <<60/100*50=30>>30 watermelons were sold.\n",
      "Thus, each watermelon sold was $80 x 30 / 60 = $<<80*30/60=20>>20.\n",
      "#### 20\n",
      "\n",
      "1: 80/100 * 50 = $<<80/100*50=40>>40\n",
      "Since 25% is 25/100 * 100% = 25%, this means that 100 - 25 = 75% of the watermelons were sold.\n",
      "Thus, each of the watermelons was sold for 40 / 75 = $<<40/75=0.4>>0.4\n",
      "#### 0.\n",
      "2: 25% of the price of 1 watermelon is $0.25 * 80 = $<<0.25*80=20>>20.\n",
      "So the total profit was $80 - $20 = $<<80-20=60>>60.\n",
      "Therefore, each watermelon was sold at $80 / 60 = $<<80/60=1.333>>1.333.\n",
      "#### 1.\n",
      "3: 80 / 25 = $<<80/25=32>>32 per watermelon\n",
      "The fruit vendor sold all of the watermelons at $32 each, which is 50 x $32 = $<<50*32=1600>>1600\n",
      "#### 1600\n",
      "\n",
      "4: 80/25 = $<<80/25=3>>3 per watermelon.\n",
      "50 * $3 = $<<50*3=150>>150\n",
      "#### 150\n",
      "\n",
      "5: 80-80=<<80-80=0>>0\n",
      "Therefore, each watermelon was sold at $80/50=$<<80/50=160>>160.\n",
      "#### 160\n",
      "\n",
      "6: 80/25 = $<<80/25=3>>3 per watermelon\n",
      "40 watermelons are sold at $3 per watermelon, for a total of 40*$3 = $<<40*3=120>>120\n",
      "50 watermelons are sold at $80 per watermelon, for a total of 50*$80 = $<<50*80=4000>>4000\n",
      "40 watermelons are sold at $3 per watermelon, for a total of 40*$3 = $<<40*3=120>>120\n",
      "So in total, the fruits were sold for $120+$4000 = $<<120+4000=4210>>4210\n",
      "#### 4210\n",
      "\n",
      "7: 80/100 * 50 = $<<80/100*50=40>>40\n",
      "40/100 * 50 = $<<40/100*50=20>>20\n",
      "20/100 * 50 = $<<20/100*50=10>>10\n",
      "The price of each watermelon was $50 - $10 = $<<50-10=40>>40\n",
      "#### 40\n",
      "\n",
      "8: 80/25 = $<<80/25=3>>3 per watermelon\n",
      "Each watermelon was sold for $3.\n",
      "#### 40\n",
      "\n",
      "9: 80/25 = <<80/25=3>>3\n",
      "The total price of the watermelons is $80. The profit is 3/5 of the total price, or a total of $80 * 0.3 = $<<80*0.3=24>>24\n",
      "Every watermelon was sold at $24, and the total number of watermelons sold is 50, so we can calculate the amount each watermelon was sold at: $24/50 = $<<24/50=0.48>>0.48 per watermelon\n",
      "#### 48\n",
      "\n",
      "10: 80-25% = <<80-25=55>>55% was the amount sold.\n",
      "50/55*80 = <<50/55*80=64>>64 was the amount sold.\n",
      "Each watermelon was sold for 80/64 = <<80/64=1>>$1.\n",
      "#### 1\n",
      "\n",
      "11: 50 x 80 / 25 = <<50*80/25=160>>160 dollar\n",
      "160 - 80 = <<160-80=80>>80 dollar\n",
      "He sold each watermelon for $<<80=80>>80\n",
      "#### 80\n",
      "\n",
      "12: 50 x 25/100 = <<50*25/100=12>>12 fruits were sold for $80.\n",
      "The price of each watermelon is $80 + $12 = $<<80+12=92>>92.\n",
      "#### 92\n",
      "\n",
      "13: 80/25=<<80/25=4>>4\n",
      "The price of each watermelon was 80 - 25=$<<80-25=55>>55\n",
      "#### 55\n",
      "\n",
      "14: 80/100 * 50 = $<<80/100*50=40>>40\n",
      "The profit on each watermelon was 40 - 25 = <<40-25=15>>15\n",
      "Each watermelon sold for $40/50 = $<<40/50=8>>8\n",
      "#### 8\n",
      "\n",
      "15: 80/0.25=<<80/0.25=320>>320\n",
      "Each watermelon was sold for $320/50=<<320/50=6>>6\n",
      "#### 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res.shape)\n",
    "\n",
    "for i in range(res.size(0)):\n",
    "    text = tokenizer.decode(res[i], skip_special_tokens=True)\n",
    "    answer_part = text.split(\"Answer: \")[-1]\n",
    "    print(f\"{i}: {answer_part}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'STOP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mSTOP\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'STOP' is not defined"
     ]
    }
   ],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood of the label: -23.194427490234375\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "label = \" Paris\"  # Likelihood를 계산하고 싶은 레이블\n",
    "\n",
    "# 전체 입력 텍스트\n",
    "input_text = prompt + label\n",
    "input_ids = tokenizer([input_text], return_tensors=\"pt\").input_ids\n",
    "label_ids = tokenizer([label], return_tensors=\"pt\").input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits  # (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "# label 토큰의 likelihood 추출\n",
    "# Prompt 길이 이후의 토큰만 고려\n",
    "label_start_idx = len(tokenizer(prompt, return_tensors=\"pt\").input_ids[0])\n",
    "logits_for_labels = logits[0, label_start_idx - 1 : -1, :]  # (label_length, vocab_size)\n",
    "\n",
    "# Label의 토큰 ID\n",
    "label_token_ids = label_ids[0]\n",
    "\n",
    "# Log-Softmax 계산\n",
    "log_probs = torch.log_softmax(logits_for_labels, dim=-1)\n",
    "\n",
    "# 각 레이블 토큰의 로그 확률 합산\n",
    "label_likelihood = log_probs[range(len(label_token_ids)-1), label_token_ids].sum()\n",
    "print(f\"Log-Likelihood of the label: {label_likelihood.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood of the label: -31.83171844482422\n"
     ]
    }
   ],
   "source": [
    "prompt = \"The capital of France is\"\n",
    "label = \" Madrid\"  # Likelihood를 계산하고 싶은 레이블\n",
    "\n",
    "# 전체 입력 텍스트\n",
    "input_text = prompt + label\n",
    "input_ids = tokenizer([input_text], return_tensors=\"pt\").input_ids\n",
    "label_ids = tokenizer([label], return_tensors=\"pt\").input_ids\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs.logits  # (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "# label 토큰의 likelihood 추출\n",
    "# Prompt 길이 이후의 토큰만 고려\n",
    "label_start_idx = len(tokenizer(prompt, return_tensors=\"pt\").input_ids[0])\n",
    "logits_for_labels = logits[0, label_start_idx - 1 : -1, :]  # (label_length, vocab_size)\n",
    "\n",
    "# Label의 토큰 ID\n",
    "label_token_ids = label_ids[0]\n",
    "\n",
    "# Log-Softmax 계산\n",
    "log_probs = torch.log_softmax(logits_for_labels, dim=-1)\n",
    "\n",
    "# 각 레이블 토큰의 로그 확률 합산\n",
    "label_likelihood = log_probs[range(len(label_token_ids)-1), label_token_ids].sum()\n",
    "print(f\"Log-Likelihood of the label: {label_likelihood.item()}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[128000,    791,   6864,    315,   9822,    374,  25048]]),\n",
       " tensor([[128000,  25048]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 128256])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_for_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128256])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([128000,  12366])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "\n",
    "def get_activation(layer_nm):\n",
    "    def hook(module, input, output):\n",
    "        if layer_nm == \"self_attn\":\n",
    "            output = output[0]\n",
    "        activation[layer_nm] = output.detach().cpu()\n",
    "    return hook\n",
    "self_attn = model.model.layers[-1].self_attn\n",
    "mlp = model.model.layers[-1].mlp\n",
    "handler1 = self_attn.register_forward_hook(get_activation('self_attn'))\n",
    "handler2 = mlp.gate_proj.register_forward_hook(get_activation('gate_proj'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "output = model(input_ids)\n",
    "handler1.remove()\n",
    "handler2.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10]), torch.Size([1, 10, 2048]), torch.Size([1, 10, 8192]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape, activation['self_attn'].shape, activation['gate_proj'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = mlp.act_fn(activation['gate_proj'][0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.7227, dtype=torch.float16),\n",
       " tensor(-0.2786, dtype=torch.float16),\n",
       " tensor(0.0792, dtype=torch.float16),\n",
       " tensor(0.3706, dtype=torch.float16))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurons.max(), neurons.min(), neurons.mean(), neurons.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(neurons.numpy(), bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0828,  0.3142,  0.0468,  ...,  0.1896,  0.0750,  0.0922],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([  41,  100,  287,  396,  402,  576,  640,  840,  870,  936, 1070, 1071,\n",
       "         1143, 1210, 1327, 1346, 1401, 1546, 1579, 1619, 1787, 1788, 1879, 1949,\n",
       "         2027, 2059, 2072, 2260, 2307, 2545, 2548, 2627, 2636, 2773, 2785, 2859,\n",
       "         2999, 3106, 3307, 3316, 3449, 3497, 3675, 3879, 4407, 4549, 4587, 4758,\n",
       "         4775, 4807, 4811, 4817, 4870, 4909, 5394, 5450, 5621, 5782, 5792, 5881,\n",
       "         5924, 6036, 6278, 6283, 6364, 6517, 6747, 6805, 6813, 6853, 6888, 7006,\n",
       "         7358, 7375, 7598, 7670, 7728, 7811, 7874, 7996, 8092, 8120]),)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres = neurons.float().quantile(0.99)\n",
    "torch.where(neurons > thres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.1087)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
